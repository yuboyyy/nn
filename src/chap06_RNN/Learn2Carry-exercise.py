#!/usr/bin/env python
# coding: utf-8

# ======================================================
# å®éªŒåç§°ï¼šRNNå­¦ä¹ å¤§æ•´æ•°åŠ æ³•çš„è¿›ä½æœºåˆ¶
# ======================================================
# æ€è·¯ï¼š
#   1. éšæœºç”Ÿæˆä¸¤ä¸ªæ•´æ•°ä½œä¸ºåŠ æ³•çš„è¾“å…¥ï¼Œè®¡ç®—å®ƒä»¬çš„å’Œã€‚
#   2. å°†æ•´æ•°æ‹†åˆ†æˆæ•°ä½ï¼ˆä½ä½åœ¨å‰ï¼Œé«˜ä½åœ¨åï¼‰ï¼Œæ–¹ä¾¿RNNé€ä½å­¦ä¹ â€œè¿›ä½â€è§„å¾‹ã€‚
#   3. æ„å»ºRNNæ¨¡å‹ï¼Œè¾“å…¥æ˜¯ä¸¤ä¸ªæ•°çš„æ•°ä½åºåˆ—ï¼Œè¾“å‡ºæ˜¯é€ä½çš„é¢„æµ‹å’Œã€‚
#   4. è®­ç»ƒæ¨¡å‹ï¼Œè®©å®ƒå­¦ä¼šæ¨¡æ‹ŸåŠ æ³•ã€‚
#
#   ğŸ“Œ RNNçš„ä¼˜åŠ¿åœ¨äºï¼šä½ä½çš„ç»“æœä¼šå½±å“é«˜ä½ï¼ˆè¿›ä½ï¼‰ï¼Œè¿™ç§æ—¶åºä¾èµ–éå¸¸é€‚åˆRNNå»ºæ¨¡ã€‚
# ======================================================

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, optimizers


# ======================================================
# ä¸€ã€æ•°æ®å¤„ç†å‡½æ•°
# ======================================================

def gen_data_batch(batch_size: int, start: int, end: int) -> tuple:
    """
    éšæœºç”Ÿæˆä¸€æ‰¹åŠ æ³•æ•°æ®

    Args:
        batch_size: æ‰¹é‡å¤§å°
        start: éšæœºæ•°èŒƒå›´ä¸‹é™ï¼ˆåŒ…å«ï¼‰
        end: éšæœºæ•°èŒƒå›´ä¸Šé™ï¼ˆä¸åŒ…å«ï¼‰

    Returns:
        (nums1, nums2, results)
            nums1: ç¬¬ä¸€ä¸ªåŠ æ•°æ•°ç»„
            nums2: ç¬¬äºŒä¸ªåŠ æ•°æ•°ç»„
            results: ä¸¤æ•°ä¹‹å’Œæ•°ç»„
    """
    nums1 = np.random.randint(start, end, batch_size)
    nums2 = np.random.randint(start, end, batch_size)
    results = nums1 + nums2
    return nums1, nums2, results


def num_to_digits(num: int) -> list:
    """æ•´æ•° â†’ æ•°ä½åˆ—è¡¨ï¼Œä¾‹å¦‚ 133412 -> [1, 3, 3, 4, 1, 2]"""
    return [int(ch) for ch in str(num)]


def digits_to_num(digits: list) -> int:
    """æ•°ä½åˆ—è¡¨ â†’ æ•´æ•°ï¼Œä¾‹å¦‚ [1, 2, 3] -> 123"""
    return int("".join(map(str, digits)))


def pad_digits(digits: list, length: int, pad: int = 0) -> list:
    """å¡«å……æ•°ä½åˆ—è¡¨åˆ°å›ºå®šé•¿åº¦ï¼ˆå³è¾¹è¡¥padï¼‰ï¼Œä¾‹å¦‚ [1,2] -> [1,2,0,0]"""
    return digits + [pad] * (length - len(digits))


def batch_prepare(nums1, nums2, results, maxlen: int):
    """
    æ‰¹é‡æ•°æ®é¢„å¤„ç†ï¼š
        1. è½¬æ¢ä¸ºæ•°ä½
        2. ç¿»è½¬æ•°ä½ï¼ˆä½ä½åœ¨å‰ï¼Œé«˜ä½åœ¨åï¼Œç¬¦åˆåŠ æ³•è§„åˆ™ï¼‰
        3. å¡«å……åˆ°å›ºå®šé•¿åº¦

    Returns:
        nums1_digits, nums2_digits, results_digits
    """
    def process_num(n):  # æå–è½¬æ¢å‡½æ•°ï¼šæ•°å­—â†’æ•°ä½â†’åè½¬â†’å¡«å……
        return pad_digits(num_to_digits(n)[::-1], maxlen)
    nums1_digits = [process_num(n) for n in nums1]
    nums2_digits = [process_num(n) for n in nums2]
    results_digits = [process_num(r) for r in results]
    return nums1_digits, nums2_digits, results_digits


def digits_batch_to_numlist(batch_digits: list) -> list:
    """æ‰¹é‡å°†é¢„æµ‹çš„æ•°ä½åˆ—è¡¨è¿˜åŸä¸ºæ•´æ•°"""
    return [digits_to_num(list(reversed(d))) for d in batch_digits]


# ======================================================
# äºŒã€æ¨¡å‹å®šä¹‰
# ======================================================

class RNNAdder(keras.Model):
    """RNNå¤§æ•°åŠ æ³•æ¨¡å‹"""

    def __init__(self):
        super().__init__()
        # åµŒå…¥å±‚ï¼šæ•°å­— 0~9 -> 32ç»´å‘é‡
        self.embed = layers.Embedding(input_dim=10, output_dim=32)

        # RNNå±‚ï¼šå­¦ä¹ è¿›ä½æœºåˆ¶
        self.rnn = layers.RNN(layers.SimpleRNNCell(64), return_sequences=True)

        # è¾“å‡ºå±‚ï¼šé¢„æµ‹æ¯ä¸ªä½ä¸Šçš„æ•°å­—ï¼ˆ0-9ï¼‰
        self.dense = layers.Dense(10)

    @tf.function
    def call(self, num1, num2):
        """
        å‰å‘ä¼ æ’­
        Args:
            num1: [batch, maxlen] ç¬¬ä¸€ä¸ªåŠ æ•°
            num2: [batch, maxlen] ç¬¬äºŒä¸ªåŠ æ•°
        Returns:
            logits: [batch, maxlen, 10] æ¯ä¸ªä½çš„é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        """
        # åµŒå…¥
        emb1 = self.embed(num1)  # [B, L, 32]
        emb2 = self.embed(num2)  # [B, L, 32]

        # æ‹¼æ¥è¾“å…¥
        x = tf.concat([emb1, emb2], axis=-1)  # [B, L, 64]

        # RNNè¾“å‡º
        rnn_out = self.rnn(x)  # [B, L, 64]

        # æ¯ä¸ªä½çš„é¢„æµ‹
        logits = self.dense(rnn_out)  # [B, L, 10]
        return logits


# ======================================================
# ä¸‰ã€è®­ç»ƒä¸è¯„ä¼°
# ======================================================

@tf.function
def compute_loss(logits, labels):
    """äº¤å‰ç†µæŸå¤±"""
    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)
    return tf.reduce_mean(losses)


@tf.function
def train_step(model, optimizer, num1, num2, labels):
    """å•æ­¥è®­ç»ƒ"""
    with tf.GradientTape() as tape:
        logits = model(num1, num2)
        loss = compute_loss(logits, labels)

    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    return loss


def train(model, optimizer, steps=1000):
    """è®­ç»ƒè¿‡ç¨‹"""
    for step in range(steps):
        # ç”Ÿæˆè®­ç»ƒæ•°æ®
        data = gen_data_batch(200, 0, 555_555_555)
        nums1, nums2, results = batch_prepare(*data, maxlen=11)

        # å•æ­¥è®­ç»ƒ
        loss = train_step(model, optimizer,
                          tf.constant(nums1, dtype=tf.int32),
                          tf.constant(nums2, dtype=tf.int32),
                          tf.constant(results, dtype=tf.int32))

        if step % 50 == 0:
            print(f"Step {step:04d}: Loss = {loss.numpy():.4f}")


def evaluate(model):
    """è¯„ä¼°æ¨¡å‹ç²¾åº¦"""
    # ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼ˆæ›´å¤§èŒƒå›´ï¼‰
    data = gen_data_batch(2000, 555_555_555, 999_999_999)
    nums1, nums2, results = batch_prepare(*data, maxlen=11)

    # é¢„æµ‹
    logits = model(tf.constant(nums1, dtype=tf.int32),
                   tf.constant(nums2, dtype=tf.int32))
    preds = np.argmax(logits.numpy(), axis=-1)

    # è½¬æ¢ä¸ºæ•´æ•°
    pred_nums = digits_batch_to_numlist(preds)

    # æ‰“å°éƒ¨åˆ†é¢„æµ‹
    for truth, pred in list(zip(data[2], pred_nums))[:20]:
        print(f"çœŸå®å€¼: {truth:<12} é¢„æµ‹å€¼: {pred:<12} æ­£ç¡®å—: {truth == pred}")

    # è®¡ç®—å‡†ç¡®ç‡
    acc = np.mean([t == p for t, p in zip(data[2], pred_nums)])
    print(f"\næ•´ä½“å‡†ç¡®ç‡: {acc:.4f}")
    return acc


# ======================================================
# å››ã€ä¸»ç¨‹åºå…¥å£
# ======================================================

if __name__ == "__main__":
    model = RNNAdder()
    optimizer = optimizers.Adam(0.001)

    print("å¼€å§‹è®­ç»ƒ...")
    train(model, optimizer, steps=3000)

    print("\næ¨¡å‹è¯„ä¼°ï¼š")
    evaluate(model)
